{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JcCWwVD3xXm"
   },
   "source": [
    "# Pre-reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Vv791_wv58d",
    "outputId": "289c9f85-f0a7-44ed-9e91-e9277c45ed51"
   },
   "outputs": [],
   "source": [
    "#!pip install mmh3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "0KC5gVN0vUr1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQFRAh6Pv_sa"
   },
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfhvKV57GLjq"
   },
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcklv4wFGUZ9"
   },
   "source": [
    " In this exercise you have to remove stopwords from a given **Initial** text and convert the text into the **Final** text given to you \n",
    "\n",
    "*Note - Please only write your code where it is mentioned to and do not change anything else*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TMIz90TGk__"
   },
   "source": [
    "**Intial text**\n",
    "\n",
    "> Sentiment analysis is the process of using natural language processing , text analysis , and statistics to analyze customer sentiment .\n",
    "\n",
    "\n",
    "**Final text**\n",
    "\n",
    "> Sentiment is process using natural language processing , text , statistics analyze customer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvSIL7XR_0AC",
    "outputId": "b390b3aa-f5c4-499f-8ec1-6ed06731247c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRF0hW7uGHGN"
   },
   "source": [
    "## Tokenize your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lsOP7BsC_8oo"
   },
   "outputs": [],
   "source": [
    "text = \"Sentiment analysis is the process of using natural language processing , text analysis , and statistics to analyze customer sentiment .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LoqIzFvdAQD3"
   },
   "outputs": [],
   "source": [
    "#write your code here to convert sentence into work tokens using function word_tokenize#\n",
    "text_tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "yxQRfHDQAcrX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'analysis', 'is', 'the', 'process', 'of', 'using', 'natural', 'language', 'processing', ',', 'text', 'analysis', ',', 'and', 'statistics', 'to', 'analyze', 'customer', 'sentiment', '.']\n"
     ]
    }
   ],
   "source": [
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVApTuyhHM_0"
   },
   "source": [
    "*Expected output*\n",
    "\n",
    "['Sentiment', 'analysis', 'is', 'the', 'process', 'of', 'using', 'natural', 'language', 'processing', ',', 'text', 'analysis', ',', 'and', 'statistics', 'to', 'analyze', 'customer', 'sentiment', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZIwl7E1HfXZ"
   },
   "source": [
    "## Creating the stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gl9hYgbGEgVg"
   },
   "outputs": [],
   "source": [
    "stopword_list = stopwords.words('english') # now stopword_list will be appended by already known common stopwords which can be seen by prinitng the list below #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "bPiEJ434K0PU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Q3zBIboZH-ct"
   },
   "outputs": [],
   "source": [
    "# Append and Remove some stopwords from the list such that after removing stopwords from intial text using stopwords_list you can get final text\n",
    "\n",
    "# ...\n",
    "# write your code here \n",
    "stopword_list.extend(['sentiment','analysis'])\n",
    "stopword_list.remove('is')\n",
    "# ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "MFh_nGN1DRpy"
   },
   "outputs": [],
   "source": [
    " #Remove all tokens from text_tokens which are also present in stopword_list and store remaining tokens in sw_filtered_text#\n",
    "\n",
    " #...\n",
    " # write your code here (Hint - create a list named sw_filtered_text fulfilling above criteria)\n",
    "sw_filtered_text=[word for word in text_tokens if word not in stopword_list]\n",
    " #.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "56k3icocDR74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Sentiment', 'is', 'process', 'using', 'natural', 'language', 'processing', ',', 'text', ',', 'statistics', 'analyze', 'customer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(sw_filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoI798IQJjRz"
   },
   "source": [
    "*Expected output*\n",
    "\n",
    "['Sentiment', 'is', 'process', 'using', 'natural', 'language', 'processing', ',', 'text', ',', 'statistics', 'analyze', 'customer', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd80wGJQAewo"
   },
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "P6MRAcAFErFM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment is process using natural language processing , text , statistics analyze customer .\n"
     ]
    }
   ],
   "source": [
    "sw_filtered_text = (\" \").join(sw_filtered_text)\n",
    "print(sw_filtered_text)\n",
    "\n",
    "\n",
    "# The above code will join all the tokens in the final list now check whether your output with the final text given to you or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKT900gTwFbo"
   },
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGMPQJMf4oZ1"
   },
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cryVbEXp4oZ2"
   },
   "source": [
    "In this exercise, with the given input, produce the given output by stemming the previously tokenized words.\n",
    "\n",
    "*Note - Write your code only where it is mentioned to and keep everything else unchanged.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD1VxLUF4oZ2"
   },
   "source": [
    "**Input**\n",
    "> ['Constantly', 'concentrating', 'nearby', 'object', 'may', 'lead', 'one', 'feel', 'strain' 'eyes']\n",
    "\n",
    "\n",
    "\n",
    "**Output**\n",
    "> ['Constant', 'concentrat', 'near', 'object', 'may', 'lead', 'on', 'feel', 'strain' 'ey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "ljE4xRlO-eOc"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "regexp = RegexpStemmer('ing$|by$|ly$|e$|es$', min=3)\n",
    "\n",
    "tokens=['Constantly', 'concentrating', 'nearby', 'object', 'may', 'lead', 'one', 'feel', 'strain' 'eyes']\n",
    "stemmed_tockens = [regexp.stem(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "BInjGAZw-qSC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'concentrat',\n",
       " 'near',\n",
       " 'object',\n",
       " 'may',\n",
       " 'lead',\n",
       " 'on',\n",
       " 'feel',\n",
       " 'strainey']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final output\n",
    "stemmed_tockens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNKHhvCW-xVS"
   },
   "source": [
    "Questionnaire:\n",
    "1. Have you written an entire function for stemming? - YES/NO -------> NO\n",
    "2. Have you used any pre-built library for this task? - YES/NO --------> YES\n",
    "3. Name the library used (write NA if not used): RegexpStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOMr50QBwHQw"
   },
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FDCBJRLvwJbq"
   },
   "source": [
    "## The Hashing Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Loy-mazWySll"
   },
   "source": [
    "The conventional Bag of Words method involves formation of vocabulary out of all the available tokens in the given text and later, building features out of the vocabulary using frequencies of tokens or just their presence (1s or 0s). Feature hashing or the hashing trick helps overcome some of the major limitations of conventional Bag of Words like growing vocabulary with growing dataset size i.e. dimensionality reduction or dodging crafty corner cases. \n",
    "\n",
    "For some further details on Feature hashing, refer to these links:\n",
    "1. https://en.wikipedia.org/wiki/Feature_hashing\n",
    "2. https://medium.com/value-stream-design/introducing-one-of-the-best-hacks-in-machine-learning-the-hashing-trick-bf6a9c8af18f\n",
    "\n",
    "For this Task, given some arbitrary data in a DataFrame, implement Feature hashing manually using MurmurHash3 (using mmh3 module imported at the beginning) to hash each word and then modulo to build a feature vector as shown in the below example:\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "| Text      | Label |\n",
    "| --------- | ----- |\n",
    "| Loved the service      | 1       |\n",
    "| Terrible food   | 0        |\n",
    "\n",
    "Using conventional bag of words, we would require 5 features for 5 unique words in both the sentences (loved, the, service, terrible, food).\n",
    "\n",
    "Let us use feature hashing such that output feature vector has 4 features (use modulo 4 after hashing)\n",
    "\n",
    "Sentence 1: Loved the service\n",
    "\n",
    "| Words      | Hash | Hash modulo 4 |\n",
    "| --------- | ----- | ------------- |\n",
    "| loved | 2334787929 | 1 |\n",
    "| the   | 3162218338 | 2 |\n",
    "| service | 3640852848 | 0 |\n",
    "\n",
    "Sentence 2: Terrible food\n",
    "\n",
    "| Words      | Hash | Hash modulo 4 |\n",
    "| --------- | ----- | ------------- |\n",
    "| terrible | 2800746226 | 2 |\n",
    "| food   | 2122679414 | 2 |\n",
    "\n",
    "Final feature vector:\n",
    "\n",
    "| Sentence      | Hash Feature 0 | Hash Feature 1 | Hash Feature 2 | Hash Feature 3 |\n",
    "| --------- | ----- | ------------- | ----- | ----- |\n",
    "| Loved the service | 1 | 1 | 1 | 0 |\n",
    "| Terrible food | 0 | 0 | 2 | 0 |\n",
    "\n",
    "Thus the output should be:\n",
    "\n",
    "\n",
    "```\n",
    "sample_output = [1, 1, 1, 0\n",
    "                 0, 0, 2, 0]\n",
    "```\n",
    "\n",
    "For the dataset given in the problem: \n",
    "\n",
    "- Required features: 1000\n",
    "\n",
    "- Hash function: mmh3 (must use unsigned hash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaAU0AMn7Jxz"
   },
   "source": [
    "**NOTE: You should only use Pandas, NumPy and mmh3 for this task, if any other library is used, your submission will not be considered.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "FiZm2ubU7XTL",
    "outputId": "f3082d27-4764-43e8-ce59-3ee82ceab9aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject enron methanol meter this is a follow ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject hpl nom for january see attached file ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject neon retreat ho ho ho we re around to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject photoshop windows office cheap main tr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject re indian springs this deal is to book...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>subject put the on the ft the transport volume...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>subject and following noms hpl can t take the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>subject calpine daily gas nomination julie as ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>subject industrial worksheets for august activ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>subject important online banking alert dear va...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5171 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_clean  label_num\n",
       "0     subject enron methanol meter this is a follow ...          0\n",
       "1     subject hpl nom for january see attached file ...          0\n",
       "2     subject neon retreat ho ho ho we re around to ...          0\n",
       "3     subject photoshop windows office cheap main tr...          1\n",
       "4     subject re indian springs this deal is to book...          0\n",
       "...                                                 ...        ...\n",
       "5166  subject put the on the ft the transport volume...          0\n",
       "5167  subject and following noms hpl can t take the ...          0\n",
       "5168  subject calpine daily gas nomination julie as ...          0\n",
       "5169  subject industrial worksheets for august activ...          0\n",
       "5170  subject important online banking alert dear va...          1\n",
       "\n",
       "[5171 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_BoW.csv',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "df['text_clean_tokens']=df['text_clean'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>label_num</th>\n",
       "      <th>text_clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject enron methanol meter this is a follow ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[subject, enron, methanol, meter, this, is, a,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject hpl nom for january see attached file ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[subject, hpl, nom, for, january, see, attache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject neon retreat ho ho ho we re around to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[subject, neon, retreat, ho, ho, ho, we, re, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject photoshop windows office cheap main tr...</td>\n",
       "      <td>1</td>\n",
       "      <td>[subject, photoshop, windows, office, cheap, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject re indian springs this deal is to book...</td>\n",
       "      <td>0</td>\n",
       "      <td>[subject, re, indian, springs, this, deal, is,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  label_num  \\\n",
       "0  subject enron methanol meter this is a follow ...          0   \n",
       "1  subject hpl nom for january see attached file ...          0   \n",
       "2  subject neon retreat ho ho ho we re around to ...          0   \n",
       "3  subject photoshop windows office cheap main tr...          1   \n",
       "4  subject re indian springs this deal is to book...          0   \n",
       "\n",
       "                                   text_clean_tokens  \n",
       "0  [subject, enron, methanol, meter, this, is, a,...  \n",
       "1  [subject, hpl, nom, for, january, see, attache...  \n",
       "2  [subject, neon, retreat, ho, ho, ho, we, re, a...  \n",
       "3  [subject, photoshop, windows, office, cheap, m...  \n",
       "4  [subject, re, indian, springs, this, deal, is,...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aaplying the hash function from mmh3\n",
    "df['text_clean_tokens']=df['text_clean_tokens'].apply(lambda x:[mmh3.hash(w,signed=False)%1000 for w in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [535, 63, 546, 876, 582, 277, 850, 75, 685, 69...\n",
       "1       [535, 688, 210, 733, 858, 112, 696, 475, 397, ...\n",
       "2       [535, 527, 621, 67, 67, 67, 791, 47, 69, 691, ...\n",
       "3       [535, 678, 204, 99, 837, 616, 604, 233, 468, 4...\n",
       "4       [535, 47, 815, 692, 582, 729, 277, 691, 254, 3...\n",
       "                              ...                        \n",
       "5166    [535, 232, 338, 185, 338, 444, 338, 312, 640, ...\n",
       "5167    [535, 451, 693, 866, 688, 309, 157, 87, 338, 3...\n",
       "5168    [535, 198, 402, 996, 828, 733, 676, 515, 390, ...\n",
       "5169    [535, 65, 193, 733, 515, 995, 696, 136, 338, 1...\n",
       "5170    [535, 551, 787, 198, 463, 587, 803, 989, 507, ...\n",
       "Name: text_clean_tokens, Length: 5171, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df['text_clean_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=[f'Hash feature {i}' for i in range(0,1000)]\n",
    "df_test=pd.DataFrame(data=None,columns=lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hash feature 0</th>\n",
       "      <th>Hash feature 1</th>\n",
       "      <th>Hash feature 2</th>\n",
       "      <th>Hash feature 3</th>\n",
       "      <th>Hash feature 4</th>\n",
       "      <th>Hash feature 5</th>\n",
       "      <th>Hash feature 6</th>\n",
       "      <th>Hash feature 7</th>\n",
       "      <th>Hash feature 8</th>\n",
       "      <th>Hash feature 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Hash feature 990</th>\n",
       "      <th>Hash feature 991</th>\n",
       "      <th>Hash feature 992</th>\n",
       "      <th>Hash feature 993</th>\n",
       "      <th>Hash feature 994</th>\n",
       "      <th>Hash feature 995</th>\n",
       "      <th>Hash feature 996</th>\n",
       "      <th>Hash feature 997</th>\n",
       "      <th>Hash feature 998</th>\n",
       "      <th>Hash feature 999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Hash feature 0, Hash feature 1, Hash feature 2, Hash feature 3, Hash feature 4, Hash feature 5, Hash feature 6, Hash feature 7, Hash feature 8, Hash feature 9, Hash feature 10, Hash feature 11, Hash feature 12, Hash feature 13, Hash feature 14, Hash feature 15, Hash feature 16, Hash feature 17, Hash feature 18, Hash feature 19, Hash feature 20, Hash feature 21, Hash feature 22, Hash feature 23, Hash feature 24, Hash feature 25, Hash feature 26, Hash feature 27, Hash feature 28, Hash feature 29, Hash feature 30, Hash feature 31, Hash feature 32, Hash feature 33, Hash feature 34, Hash feature 35, Hash feature 36, Hash feature 37, Hash feature 38, Hash feature 39, Hash feature 40, Hash feature 41, Hash feature 42, Hash feature 43, Hash feature 44, Hash feature 45, Hash feature 46, Hash feature 47, Hash feature 48, Hash feature 49, Hash feature 50, Hash feature 51, Hash feature 52, Hash feature 53, Hash feature 54, Hash feature 55, Hash feature 56, Hash feature 57, Hash feature 58, Hash feature 59, Hash feature 60, Hash feature 61, Hash feature 62, Hash feature 63, Hash feature 64, Hash feature 65, Hash feature 66, Hash feature 67, Hash feature 68, Hash feature 69, Hash feature 70, Hash feature 71, Hash feature 72, Hash feature 73, Hash feature 74, Hash feature 75, Hash feature 76, Hash feature 77, Hash feature 78, Hash feature 79, Hash feature 80, Hash feature 81, Hash feature 82, Hash feature 83, Hash feature 84, Hash feature 85, Hash feature 86, Hash feature 87, Hash feature 88, Hash feature 89, Hash feature 90, Hash feature 91, Hash feature 92, Hash feature 93, Hash feature 94, Hash feature 95, Hash feature 96, Hash feature 97, Hash feature 98, Hash feature 99, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 1000 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject enron methanol meter this is a follow ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject hpl nom for january see attached file ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject neon retreat ho ho ho we re around to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject photoshop windows office cheap main tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject re indian springs this deal is to book...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>subject put the on the ft the transport volume...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>subject and following noms hpl can t take the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>subject calpine daily gas nomination julie as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>subject industrial worksheets for august activ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>subject important online banking alert dear va...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5171 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             text_clean\n",
       "0     subject enron methanol meter this is a follow ...\n",
       "1     subject hpl nom for january see attached file ...\n",
       "2     subject neon retreat ho ho ho we re around to ...\n",
       "3     subject photoshop windows office cheap main tr...\n",
       "4     subject re indian springs this deal is to book...\n",
       "...                                                 ...\n",
       "5166  subject put the on the ft the transport volume...\n",
       "5167  subject and following noms hpl can t take the ...\n",
       "5168  subject calpine daily gas nomination julie as ...\n",
       "5169  subject industrial worksheets for august activ...\n",
       "5170  subject important online banking alert dear va...\n",
       "\n",
       "[5171 rows x 1 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test1=pd.DataFrame(data=df['text_clean'])\n",
    "df_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatinating both the dataframes\n",
    "df_test=pd.concat([df_test1,df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Hash feature 0</th>\n",
       "      <th>Hash feature 1</th>\n",
       "      <th>Hash feature 2</th>\n",
       "      <th>Hash feature 3</th>\n",
       "      <th>Hash feature 4</th>\n",
       "      <th>Hash feature 5</th>\n",
       "      <th>Hash feature 6</th>\n",
       "      <th>Hash feature 7</th>\n",
       "      <th>Hash feature 8</th>\n",
       "      <th>...</th>\n",
       "      <th>Hash feature 990</th>\n",
       "      <th>Hash feature 991</th>\n",
       "      <th>Hash feature 992</th>\n",
       "      <th>Hash feature 993</th>\n",
       "      <th>Hash feature 994</th>\n",
       "      <th>Hash feature 995</th>\n",
       "      <th>Hash feature 996</th>\n",
       "      <th>Hash feature 997</th>\n",
       "      <th>Hash feature 998</th>\n",
       "      <th>Hash feature 999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject enron methanol meter this is a follow ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject hpl nom for january see attached file ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject neon retreat ho ho ho we re around to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject photoshop windows office cheap main tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject re indian springs this deal is to book...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  Hash feature 0  \\\n",
       "0  subject enron methanol meter this is a follow ...               0   \n",
       "1  subject hpl nom for january see attached file ...               0   \n",
       "2  subject neon retreat ho ho ho we re around to ...               0   \n",
       "3  subject photoshop windows office cheap main tr...               0   \n",
       "4  subject re indian springs this deal is to book...               0   \n",
       "\n",
       "   Hash feature 1  Hash feature 2  Hash feature 3  Hash feature 4  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   Hash feature 5  Hash feature 6  Hash feature 7  Hash feature 8  ...  \\\n",
       "0               0               0               0               0  ...   \n",
       "1               0               0               0               0  ...   \n",
       "2               0               0               0               0  ...   \n",
       "3               0               0               0               0  ...   \n",
       "4               0               0               0               0  ...   \n",
       "\n",
       "   Hash feature 990  Hash feature 991  Hash feature 992  Hash feature 993  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   Hash feature 994  Hash feature 995  Hash feature 996  Hash feature 997  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   Hash feature 998  Hash feature 999  \n",
       "0                 0                 0  \n",
       "1                 0                 0  \n",
       "2                 0                 0  \n",
       "3                 0                 0  \n",
       "4                 0                 0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filling the dataframe with 0's initially\n",
    "df_test.fillna(value=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for adding the values in dataframe as per their occurences in the sentence\n",
    "for i in range(len(df)):\n",
    "    lst=df['text_clean_tokens'][i]\n",
    "    for j in lst:\n",
    "        label=f'Hash feature {j}'\n",
    "        df_test.loc[i,label]=df_test.loc[i,label]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_clean</th>\n",
       "      <th>Hash feature 0</th>\n",
       "      <th>Hash feature 1</th>\n",
       "      <th>Hash feature 2</th>\n",
       "      <th>Hash feature 3</th>\n",
       "      <th>Hash feature 4</th>\n",
       "      <th>Hash feature 5</th>\n",
       "      <th>Hash feature 6</th>\n",
       "      <th>Hash feature 7</th>\n",
       "      <th>Hash feature 8</th>\n",
       "      <th>...</th>\n",
       "      <th>Hash feature 990</th>\n",
       "      <th>Hash feature 991</th>\n",
       "      <th>Hash feature 992</th>\n",
       "      <th>Hash feature 993</th>\n",
       "      <th>Hash feature 994</th>\n",
       "      <th>Hash feature 995</th>\n",
       "      <th>Hash feature 996</th>\n",
       "      <th>Hash feature 997</th>\n",
       "      <th>Hash feature 998</th>\n",
       "      <th>Hash feature 999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>subject enron methanol meter this is a follow ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subject hpl nom for january see attached file ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>subject neon retreat ho ho ho we re around to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>subject photoshop windows office cheap main tr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subject re indian springs this deal is to book...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          text_clean  Hash feature 0  \\\n",
       "0  subject enron methanol meter this is a follow ...               0   \n",
       "1  subject hpl nom for january see attached file ...               0   \n",
       "2  subject neon retreat ho ho ho we re around to ...               1   \n",
       "3  subject photoshop windows office cheap main tr...               0   \n",
       "4  subject re indian springs this deal is to book...               0   \n",
       "\n",
       "   Hash feature 1  Hash feature 2  Hash feature 3  Hash feature 4  \\\n",
       "0               0               0               0               0   \n",
       "1               0               0               0               0   \n",
       "2               1               0               0               0   \n",
       "3               0               0               0               0   \n",
       "4               0               0               0               0   \n",
       "\n",
       "   Hash feature 5  Hash feature 6  Hash feature 7  Hash feature 8  ...  \\\n",
       "0               0               0               0               0  ...   \n",
       "1               0               0               0               0  ...   \n",
       "2               0               0               0               0  ...   \n",
       "3               0               0               0               0  ...   \n",
       "4               0               0               0               0  ...   \n",
       "\n",
       "   Hash feature 990  Hash feature 991  Hash feature 992  Hash feature 993  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 1                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   Hash feature 994  Hash feature 995  Hash feature 996  Hash feature 997  \\\n",
       "0                 0                 1                 1                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 1                 2                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   Hash feature 998  Hash feature 999  \n",
       "0                 0                 0  \n",
       "1                 0                 0  \n",
       "2                 0                 4  \n",
       "3                 0                 0  \n",
       "4                 1                 0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 1, 0, ..., 0, 0, 4],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [7, 0, 0, ..., 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy array as per the sample c\n",
    "np.array(df_test.loc[:,'Hash feature 0':])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mid-evaluation Tasks [WoC 4.0: Sentiment Analysis].ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
